# Разработка локального поискового движка по сайту

## Введение

Итоговый проект курса Java. Необходимо разработать поисковый движок для информационно-новостного портала.

## Описание проекта

Разработка Spring-приложения (JAR-файл) с локальной базой данных MySQL, веб-интерфейсом и API для управления и получения результатов поиска.

### Принципы работы

1.  Конфигурация адресов сайтов в файле.
2.  Самостоятельный обход и индексация страниц.
3.  Прием запросов через API.
4.  Трансформация запроса в список лемм.
5.  Поиск страниц по леммам в индексе.
6.  Ранжирование и сортировка результатов.

## Навыки

*   Написание кода на Java.
*   Синтаксис Java.
*   Понятный и поддерживаемый код.
*   Работа с числами, строками, датами и коллекциями.
*   Создание классов, абстрактных классов и интерфейсов.
*   Работа с файлами CSV, HTML и JSON.
*   Многопоточные приложения.
*   Работа с исключениями и логами.
*   Приложения на Spring Boot.
*   Поиск решений проблем в интернете.

## Рекомендации по работе над проектом

*   Прочитать техническое задание и спецификацию.
*   Планировать работу небольшими подходами по 2-3 часа.
*   Проверять проект самостоятельно перед отправкой.
*   Привлекать друзей/коллег для тестирования.

## Рекомендации по технической реализации

1.  Принципы «чистого» кода:
    *   Избегать повторов кода.
    *   Именование переменных, методов и классов.
    *   Методы не длиннее 30 строк кода, не более трёх параметров.
    *   Избегать высокой вложенности кода (не более двух уровней).
    *   Упрощать код.
    *   Использовать тернарный оператор.
    *   Использовать `min/max` для сравнения.
    *   Минимизировать комментарии в коде.
2.  Работа с базой данных:
    *   Избегать запросов без ограничений.
    *   Избегать многократных запросов в циклах.

## Этапы реализации

### Этап 1. Подготовка

*   Установка JDK и IntelliJ IDEA.
*   Загрузка проекта-заготовки из Git-репозитория.
*   Запуск приложения и открытие в браузере (http://localhost:8080/).
*   Изучение структуры проекта (Spring Boot, Maven, Thymeleaf, Lombok).
*   Изучение работы контроллеров (DefaultController, ApiController) и сервисов (StatisticsService).
*   Установка MySQL-сервера и создание базы данных `search_engine`.
*   Подключение зависимостей для работы с базой данных.
*   Настройка `application.yaml` с данными доступа к MySQL.

### Этап 2. Система обхода веб-страниц

*   Реализация многопоточного приложения для обхода сайтов и сохранения содержимого в БД.
*   Создание классов для таблиц `site` и `page` (папка `model`).
*   Установка индекса по полю `path` в таблице `page`.
*   Создание метода запуска индексации `startIndexing` в контроллере.
*   Реализация сервиса индексации сайтов:
    *   Удаление данных по сайту.
    *   Создание записи в `site` со статусом `INDEXING`.
    *   Обход страниц, добавление в `page`.
    *   Обновление `status_time` в `site`.
    *   Изменение статуса на `INDEXED` или `FAILED`.
*   Использование Fork-Join для создания потоков.
*   Проверка наличия ссылки в БД перед переходом.
*   Определение кода ответа (JSOUP).
*   Использование фейкового User-Agent и referrer.
*   Задержки между запросами (0,5–5 секунд).
*   Реализация функции остановки обхода `stopIndexing`.

### Этап 3. Система индексации веб-страниц

*   Реализация системы индексации страниц для определения релевантных страниц.
*   Создание классов для таблиц `lemma` и `index`.
*   Подключение библиотек для лемматизации (LuceneMorphology).
*   Создание метода для получения перечня лемм из текста.
*   Реализация функции индексации отдельной веб-страницы (`indexPage`).
*   Сохранение HTML-кода в `page`, лемм и их количества в `lemma` и `index`.
*   Обработка ошибок при индексации страниц с других сайтов.
*   Удаление информации о странице перед повторной индексацией.
*   Доработка кода обхода веб-страниц для сохранения лемм и их количества.
*   Исключение индексации страниц с ошибочными HTTP-кодами (4xx, 5xx).
*   Переработка метода API `statistics`.

### Этап 4. Система поиска

*   Реализация системы поиска информации с использованием поискового индекса.
*   Создание метода для команды API `search`.
*   Разбиение поискового запроса на леммы.
*   Исключение лемм, встречающихся на большом количестве страниц.
*   Сортировка лемм по частоте встречаемости.
*   Поиск страниц по леммам.
*   Расчет абсолютной и относительной релевантности.
*   Сортировка страниц по убыванию релевантности.
*   Формирование сниппетов с выделением совпадений.
*   Учет сайтов для поиска (все или выбранный).

### Этап 5. Публикация проекта на GitHub

*   Размещение исходных кодов на GitHub.
*   Создание файла `README.md` с описанием проекта, стеком технологий и инструкцией по запуску.
